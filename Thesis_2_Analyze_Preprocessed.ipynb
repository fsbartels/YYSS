{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis 2. Analyze Preprocessed.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wv0iyb3trVa",
        "colab_type": "text"
      },
      "source": [
        "## KEY VARIABLES\n",
        "* Youtube_ID obviously designates the video to analyze.\n",
        "* Frame step designates how many frames per second to analyze with OpenPose. Default fps is 24 frames per second.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0qXCFKSttMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "YOUTUBE_ID = 'Ci3na6ThUJc'\n",
        "# Frame_step also has to be set manually @ preprocessing!\n",
        "FRAMESTEP = 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6qgyzjjuvZa",
        "colab_type": "text"
      },
      "source": [
        "# **Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk56Wvpf508P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "# Pick and show Youtube video\n",
        "### Elderly yoga example: YOUTUBE_ID = 'kFhG-ZzLNN4'\n",
        "### Default example: YOUTUBE_ID = 'nQFf38xeBww'\n",
        "\n",
        "#Import required modules\n",
        "import pandas as pd\n",
        "from pandas.io.json import json_normalize\n",
        "from pathlib import Path\n",
        "import json\n",
        "from IPython.display import YouTubeVideo\n",
        "from google.colab import drive\n",
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import argparse\n",
        "import glob\n",
        "import imutils\n",
        "import seaborn as sns; sns.set()\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "import PIL\n",
        "import warnings\n",
        "from scipy.spatial import procrustes \n",
        "import sys\n",
        "import seaborn as sns\n",
        "import scipy as sp\n",
        "from scipy.spatial import distance\n",
        "from scipy.signal import savgol_filter\n",
        "import sklearn\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from PIL import Image\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageDraw\n",
        "from scipy.signal import argrelextrema\n",
        "\n",
        "#Options\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
        "\n",
        "#Mount google drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "YouTubeVideo(YOUTUBE_ID)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5pzVHvGBSHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load preprocessed pose information.\n",
        "\n",
        "path = r'/content/drive/My Drive/Thesis/Poseinformation/'\n",
        "all_files = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "li = []\n",
        "\n",
        "for filename in all_files:\n",
        "  df = pd.read_csv(filename, index_col=None, header=0)\n",
        "  li.append(df)\n",
        "\n",
        "poseinformation = pd.concat(li, axis=0, ignore_index=True)\n",
        "\n",
        "\n",
        "print(poseinformation.shape)\n",
        "print(len(filename))\n",
        "\n",
        "poseinformation.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBLOFEo7SyNb",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hT-69JdMTPQ",
        "colab_type": "text"
      },
      "source": [
        "# Shot Boundary Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KAGCmBgMPzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Process files for shot boundary detection or load a preprocessed file if there is one.\n",
        "\n",
        "#Thanks to: https://docs.opencv.org/3.4/d8/dc8/tutorial_histogram_comparison.html\n",
        "#Thanks to: https://www.pyimagesearch.com/2014/07/14/3-ways-compare-histograms-using-opencv-python/\n",
        "\n",
        "if os.path.isfile('/content/drive/My Drive/Thesis/SBD_' + YOUTUBE_ID + '_preprocessed.csv') == True:\n",
        "  df_HistCompare = pd.read_csv('/content/drive/My Drive/Thesis/SBD_' + YOUTUBE_ID + '_preprocessed.csv', sep=',')\n",
        "  print('Loaded preprocessed file')\n",
        "else:\n",
        "  filenames = []\n",
        "  for root, dirs, files in os.walk(r'/content/drive/My Drive/Thesis/jpg_output/'):\n",
        "      for file in files:\n",
        "          if file.endswith('.png'):\n",
        "              filenames.append(file)\n",
        "  print(\"Number of files to analyze:\", len(filenames))\n",
        "  df_HistCompare = pd.DataFrame({'Method': [], 'Base_Base': [], 'Base_Test1': [], 'Image1': [], 'Image2': []})\n",
        "\n",
        "  num_frames = sum([len(files) for r, d, files in os.walk(\"/content/drive/My Drive/Thesis/jpg_output\")])\n",
        "\n",
        "  i = 0\n",
        "  for filenames[i] in filenames:\n",
        "    print('File ', i, ' of ', num_frames)\n",
        "    image1 = \"/content/drive/My Drive/Thesis/jpg_output/\" + filenames[i]\n",
        "    #Exception handling in case we've gone through all files.\n",
        "    try:\n",
        "      image2 = \"/content/drive/My Drive/Thesis/jpg_output/\" + filenames[i+1]\n",
        "    except IndexError:\n",
        "      break\n",
        "\n",
        "    #print(image1, image2)\n",
        "\n",
        "    src_base = cv.imread(image1)\n",
        "    src_test1 = cv.imread(image2)\n",
        "\n",
        "    if src_base is None or src_test1 is None:# or src_test2 is None:\n",
        "        print('Could not open or find the images!')\n",
        "        continue\n",
        "\n",
        "    ## [Convert to HSV]\n",
        "    hsv_base = cv.cvtColor(src_base, cv.COLOR_BGR2HSV)\n",
        "    hsv_test1 = cv.cvtColor(src_test1, cv.COLOR_BGR2HSV)\n",
        "    #hsv_test2 = cv.cvtColor(src_test2, cv.COLOR_BGR2HSV)\n",
        "    ## [Convert to HSV]\n",
        "\n",
        "    ## [Convert to HSV half]\n",
        "    hsv_half_down = hsv_base[hsv_base.shape[0]//2:,:]\n",
        "    ## [Convert to HSV half]\n",
        "\n",
        "    ## [Using 50 bins for hue and 60 for saturation]\n",
        "    h_bins = 50\n",
        "    s_bins = 60\n",
        "    histSize = [h_bins, s_bins]\n",
        "\n",
        "    # hue varies from 0 to 179, saturation from 0 to 255\n",
        "    h_ranges = [0, 180]\n",
        "    s_ranges = [0, 256]\n",
        "    ranges = h_ranges + s_ranges # concat lists\n",
        "\n",
        "    # Use the 0-th and 1-st channels\n",
        "    channels = [0, 1]\n",
        "    ## [Using 50 bins for hue and 60 for saturation]\n",
        "\n",
        "    ## [Calculate the histograms for the HSV images]\n",
        "    hist_base = cv.calcHist([hsv_base], channels, None, histSize, ranges, accumulate=False)\n",
        "    cv.normalize(hist_base, hist_base, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n",
        "\n",
        "    hist_half_down = cv.calcHist([hsv_half_down], channels, None, histSize, ranges, accumulate=False)\n",
        "    cv.normalize(hist_half_down, hist_half_down, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n",
        "\n",
        "    hist_test1 = cv.calcHist([hsv_test1], channels, None, histSize, ranges, accumulate=False)\n",
        "    cv.normalize(hist_test1, hist_test1, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n",
        "\n",
        "    #hist_test2 = cv.calcHist([hsv_test2], channels, None, histSize, ranges, accumulate=False)\n",
        "    #cv.normalize(hist_test2, hist_test2, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n",
        "    ## [Calculate the histograms for the HSV images]\n",
        "\n",
        "    ## [Apply the histogram comparison methods]\n",
        "    for compare_method in range(4):\n",
        "        base_base = cv.compareHist(hist_base, hist_base, compare_method)\n",
        "        #base_half = cv.compareHist(hist_base, hist_half_down, compare_method)\n",
        "        base_test1 = cv.compareHist(hist_base, hist_test1, compare_method)\n",
        "        #base_test2 = cv.compareHist(hist_base, hist_test2, compare_method)\n",
        "\n",
        "        #print('Method:', compare_method, 'Perfect, Base-Half, Base-Test(1):',\\\n",
        "        #      base_base, '/', base_half, '/', base_test1, '/', asps[i], asps[i+1])# '/', base_test2)\n",
        "        df_HistCompare = df_HistCompare.append({'Method': compare_method, 'Base_Base': base_base, 'Base_Test1': base_test1, 'Image1': filenames[i], 'Image2': filenames[i+1]}, ignore_index=True)\n",
        "    i += 1\n",
        "    if i % 10 == 0:\n",
        "      clear_output()\n",
        "\n",
        "  ##Optionally save to file\n",
        "  df_HistCompare.to_csv(path_or_buf='/content/drive/My Drive/Thesis/SBD_' + YOUTUBE_ID + '_preprocessed.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_MbpoVc0wZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check initial results\n",
        "df_HistCompare.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBLsntTDM-j6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_HistCompare = pd.read_csv('drive/My Drive/Thesis/openpose.avi_r8904530ijyiopf9034jiop4g90j0yh795640h38j/frames_elderly_preprocessed.csv')\n",
        "\n",
        "#Prepare dataframes for plotting of different methods (use set options to investigate)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.max_columns', 1000)\n",
        "\n",
        "df_HistCompare_Correlation = df_HistCompare[df_HistCompare['Method']==0.0]\n",
        "df_HistCompare_Chisquare = df_HistCompare[df_HistCompare['Method']==1.0]\n",
        "df_HistCompare_Intersection = df_HistCompare[df_HistCompare['Method']==2.0]\n",
        "df_HistCompare_Bhattacharyya = df_HistCompare[df_HistCompare['Method']==3.0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAC_vEKJNBy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_HistCompare_Correlation.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7TvQrPGNDwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Thanks to: https://stackoverflow.com/questions/39951196/how-to-find-values-below-or-above-average\n",
        "def zscore(s):\n",
        "    return (s - np.mean(s)) / np.std(s)\n",
        "df_HistCompare_Correlation_Cuts = df_HistCompare_Correlation[zscore(df_HistCompare_Correlation['Base_Test1']).abs().gt(2)]\n",
        "#print(df_HistCompare_Correlation[zscore(df_HistCompare_Correlation['Base_Test1']).abs().gt(2)])\n",
        "df_HistCompare_Chisquare_Cuts = df_HistCompare_Chisquare[zscore(df_HistCompare_Chisquare['Base_Test1']).abs().gt(2)]\n",
        "#print(df_HistCompare_Chisquare[zscore(df_HistCompare_Chisquare['Base_Test1']).abs().gt(2)])\n",
        "df_HistCompare_Intersection_Cuts = df_HistCompare_Intersection[zscore(df_HistCompare_Intersection['Base_Test1']).abs().gt(2)]\n",
        "#print(df_HistCompare_Intersection[zscore(df_HistCompare_Intersection['Base_Test1']).abs().gt(2)])\n",
        "df_HistCompare_Bhattacharyya_Cuts = df_HistCompare_Bhattacharyya[zscore(df_HistCompare_Bhattacharyya['Base_Test1']).abs().gt(2)]\n",
        "#print(df_HistCompare_Bhattacharyya[zscore(df_HistCompare_Bhattacharyya['Base_Test1']).abs().gt(2)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h64zjEOnRzi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_HistCompare_Correlation_Cuts['Image2']\n",
        "df_HistCompare_Correlation_Cuts['Imagename']  = '/content/drive/My Drive/Thesis/jpg_output/' + df_HistCompare_Correlation_Cuts['Image2'].astype(str)\n",
        "df_HistCompare_Correlation_Cuts.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQDR1w_LviIW",
        "colab_type": "text"
      },
      "source": [
        "# **Distance calculations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIOaWIdjPrgg",
        "colab_type": "text"
      },
      "source": [
        "##Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQrM47kY50X2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set column names correctly.\n",
        "poseinformation.columns = ['Unnamed', 'Nose_X', 'Nose_Y', 'Nose_Conf', 'Neck_X', 'Neck_Y', 'Neck_Conf', 'Rshoulder_X', 'Rshoulder_Y', 'Rshoulder_Conf', 'Relbow_X', 'Relbow_Y', 'Relbow_Conf', 'Rwrist_X', 'Rwrist_Y', 'Rwrist_Conf', 'Lshoulder_X', 'Lshoulder_Y', 'Lshoulder_Conf', 'Lelbow_X', 'Lelbow_Y', 'Lelbow_Conf', 'Lwrist_X', 'Lwrist_Y', 'Lwrist_Conf', 'MidHip_X', 'MidHip_Y', 'MidHip_Conf', 'Rhip_X', 'Rhip_Y', 'Rhip_Conf', 'Rknee_X', 'Rknee_Y', 'Rknee_Conf', 'Rankle_X', 'Rankle_Y', 'Rankle_Conf', 'Lhip_X', 'Lhip_Y', 'Lhip_Conf', 'Lknee_X', 'Lknee_Y', 'Lknee_Conf', 'Lankle_X', 'Lankle_Y', 'Lankle_Conf', 'Reye_X', 'Reye_Y', 'Reye_Conf', 'Leye_X', 'Leye_Y', 'Leye_Conf', 'Rear_X', 'Rear_Y', 'Rear_Conf', 'Lear_X', 'Lear_Y', 'Lear_Conf', 'Lbigtoe_X', 'Lbigtoe_Y', 'Lbigtoe_Conf', 'Lsmalltoe_X', 'Lsmalltoe_Y', 'Lsmalltoe_Conf', 'Lheel_X', 'Lheel_Y', 'Lheel_Conf', 'Rbigtoe_X', 'Rbigtoe_Y', 'Rbigtoe_Conf', 'Rsmalltoe_X', 'Rsmalltoe_Y', 'Rsmalltoe_Conf', 'Rheel_X', 'Rheel_Y', 'Rheel_Conf', 'Filename', 'Imagename']\n",
        "\n",
        "#Optionally save to file\n",
        "poseinformation.to_csv(path_or_buf='/content/drive/My Drive/Thesis/json_preprocessed_'+YOUTUBE_ID+'.csv')\n",
        "\n",
        "\n",
        "poseinformation_clean = poseinformation.replace(0,np.NaN)\n",
        "poseinformation_clean = poseinformation_clean.dropna(thresh=50)\n",
        "poseinformation_clean\n",
        "\n",
        "percent_missing = poseinformation_clean.isnull().sum() * 100 / len(poseinformation_clean)\n",
        "missing_value_df = pd.DataFrame({'column_name': poseinformation_clean.columns,\n",
        "                                 'percent_missing': percent_missing})\n",
        "missing_value_df = missing_value_df.sort_values(by='percent_missing', ascending=False)\n",
        "\n",
        "missing_value_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EYI_X0_CsS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create dataframe with only XY coordinates.\n",
        "\n",
        "temp7 = poseinformation[['Nose_X', 'Nose_Y', \n",
        "                        'Neck_X', 'Neck_Y',\n",
        "                        'Rshoulder_X', 'Rshoulder_Y',\n",
        "                        'Relbow_X', 'Relbow_Y',\n",
        "                        'Rwrist_X', 'Rwrist_Y', \n",
        "                        'Lshoulder_X','Lshoulder_Y',\n",
        "                        'Lelbow_X', 'Lelbow_Y',\n",
        "                        'Lwrist_X', 'Lwrist_Y',\n",
        "                        'MidHip_X', 'MidHip_Y',\n",
        "                        'Rhip_X', 'Rhip_Y',\n",
        "                        'Rknee_X', 'Rknee_Y',\n",
        "                        'Rankle_X', 'Rankle_Y', \n",
        "                        'Lhip_X', 'Lhip_Y',\n",
        "                        'Lknee_X', 'Lknee_Y',\n",
        "                        'Lankle_X', 'Lankle_Y',\n",
        "                        'Reye_X', 'Reye_Y',\n",
        "                        'Leye_X', 'Leye_Y',\n",
        "                        'Rear_X', 'Rear_Y',\n",
        "                        'Lear_X', 'Lear_Y',\n",
        "                        'Lbigtoe_X', 'Lbigtoe_Y',\n",
        "                        'Lsmalltoe_X', 'Lsmalltoe_Y',\n",
        "                        'Lheel_X', 'Lheel_Y', \n",
        "                        'Rbigtoe_X', 'Rbigtoe_Y',\n",
        "                        'Rsmalltoe_X', 'Rsmalltoe_Y',\n",
        "                        'Rheel_X', 'Rheel_Y']]\n",
        "\n",
        "\n",
        "temp8 = temp7.replace(0.000,np.NaN)\n",
        "\n",
        "percent_missing = temp8.isnull().sum() * 100 / len(temp8)\n",
        "missing_value_df = pd.DataFrame({'column_name': temp8.columns,\n",
        "                                 'percent_missing': percent_missing})\n",
        "missing_value_df.sort_values('percent_missing', ascending= False, inplace=True)\n",
        "missing_value_df\n",
        "temp7_normalized = preprocessing.normalize(temp7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0pyjd8F9PG5z",
        "colab": {}
      },
      "source": [
        "#Thanks to: https://ieeexplore-ieee-org.proxy.uba.uva.nl:2443/stamp/stamp.jsp?tp=&arnumber=1280752\n",
        "i = 0\n",
        "\n",
        "braycurtis_List = []\n",
        "canberra_List = []\n",
        "chebyshev_List = []\n",
        "cityblock_List = []\n",
        "correlation_List = []\n",
        "cosine_List = []\n",
        "euclidean_List = []\n",
        "jensenshannon_List = []\n",
        "mahalanobis_List = []\n",
        "minkowski_List = []\n",
        "seuclidean_List = []\n",
        "sqeuclidean_List = []\n",
        "wminkowski_List = []\n",
        "dice_List = []\n",
        "hamming_List = []\n",
        "jaccard_List = []\n",
        "kulsinski_List = []\n",
        "rogerstanimoto_List = []\n",
        "russellrao_List = []\n",
        "sokalmichener_List = []\n",
        "sokalsneath_List = []\n",
        "yule_List = []\n",
        "\n",
        "temp7 = temp7_normalized#.values\n",
        "\n",
        "temp7_var = np.var(temp7_normalized, axis=0)\n",
        "print(temp7_normalized.shape)\n",
        "covmx = np.cov(temp7_normalized, rowvar=False)\n",
        "print(covmx.shape)\n",
        "invcovmx = sp.linalg.inv(covmx)\n",
        "print(invcovmx.shape)\n",
        "\n",
        "for temp7[i] in temp7:\n",
        "  print('Row ', i,)\n",
        "  a = temp7[i, 0:50]\n",
        "  try:\n",
        "    b = temp7[i+1, 0:50]\n",
        "  except IndexError:\n",
        "    break\n",
        "  #a = np.reshape(a, (-1, 2))\n",
        "  #b = np.reshape(b, (-1, 2))\n",
        "  try:\n",
        "    braycurtis_Temp = distance.braycurtis(a, b)\n",
        "    canberra_Temp = distance.canberra(a, b)\n",
        "    chebyshev_Temp = distance.chebyshev(a, b)\n",
        "    cityblock_Temp = distance.cityblock(a, b)\n",
        "    correlation_Temp = distance.correlation(a, b)\n",
        "    cosine_Temp = distance.cosine(a, b)\n",
        "    euclidean_Temp = distance.euclidean(a, b)\n",
        "    jensenshannon_Temp = distance.jensenshannon(a, b)\n",
        "    mahalanobis_Temp = distance.mahalanobis(a, b, invcovmx)\n",
        "    minkowski_Temp = distance.minkowski(a, b)\n",
        "    seuclidean_Temp = distance.seuclidean(a, b, temp7_var)\n",
        "    sqeuclidean_Temp = distance.sqeuclidean(a, b)\n",
        "    #wminkowski_Temp = distance.wminkowski(a, b)\n",
        "\n",
        "    dice_Temp = distance.dice(a, b)\n",
        "    hamming_Temp = distance.hamming(a, b)\n",
        "    jaccard_Temp = distance.jaccard(a, b)\n",
        "    kulsinski_Temp = distance.kulsinski(a, b)\n",
        "    rogerstanimoto_Temp = distance.rogerstanimoto(a, b)\n",
        "    russellrao_Temp = distance.russellrao(a, b)\n",
        "    sokalmichener_Temp = distance.sokalmichener(a, b)\n",
        "    sokalsneath_Temp = distance.sokalsneath(a, b)\n",
        "    yule_Temp = distance.yule(a, b)\n",
        "  except ZeroDivisionError:\n",
        "    continue\n",
        "\n",
        "  braycurtis_List.append(braycurtis_Temp)\n",
        "  canberra_List.append(canberra_Temp)\n",
        "  chebyshev_List.append(chebyshev_Temp)\n",
        "  cityblock_List.append(cityblock_Temp)\n",
        "  correlation_List.append(correlation_Temp)\n",
        "  cosine_List.append(cosine_Temp)\n",
        "  euclidean_List.append(euclidean_Temp)\n",
        "  jensenshannon_List.append(jensenshannon_Temp)\n",
        "  mahalanobis_List.append(mahalanobis_Temp)\n",
        "  minkowski_List.append(minkowski_Temp)\n",
        "  seuclidean_List.append(seuclidean_Temp)\n",
        "  sqeuclidean_List.append(sqeuclidean_Temp)\n",
        "  #wminkowski_List.append(wminkowski_Temp)\n",
        "  \n",
        "  dice_List.append(dice_Temp)\n",
        "  hamming_List.append(hamming_Temp)\n",
        "  jaccard_List.append(jaccard_Temp)\n",
        "  kulsinski_List.append(kulsinski_Temp)\n",
        "  rogerstanimoto_List.append(rogerstanimoto_Temp)\n",
        "  russellrao_List.append(russellrao_Temp)\n",
        "  sokalmichener_List.append(sokalmichener_Temp)\n",
        "  sokalsneath_List.append(sokalsneath_Temp)\n",
        "  yule_List.append(yule_Temp)\n",
        "\n",
        "  i += 1\n",
        "  if i % 10 == 0:\n",
        "    clear_output()\n",
        "\n",
        "    \n",
        "#Fix first value.  \n",
        "braycurtis_List.insert(0,0)\n",
        "canberra_List.insert(0,0)\n",
        "chebyshev_List.insert(0,0)\n",
        "cityblock_List.insert(0,0)\n",
        "correlation_List.insert(0,0)\n",
        "cosine_List.insert(0,0)\n",
        "euclidean_List.insert(0,0)\n",
        "jensenshannon_List.insert(0,0)\n",
        "mahalanobis_List.insert(0,0)\n",
        "minkowski_List.insert(0,0)\n",
        "seuclidean_List.insert(0,0)\n",
        "sqeuclidean_List.insert(0,0)\n",
        "dice_List.insert(0,0)\n",
        "hamming_List.insert(0,0)\n",
        "jaccard_List.insert(0,0)\n",
        "kulsinski_List.insert(0,0)\n",
        "rogerstanimoto_List.insert(0,0)\n",
        "russellrao_List.insert(0,0)\n",
        "sokalmichener_List.insert(0,0)\n",
        "sokalsneath_List.insert(0,0)\n",
        "yule_List.insert(0,0)\n",
        "\n",
        "#Store results in dataframe.\n",
        "poseinformation['braycurtis'] = braycurtis_List\n",
        "poseinformation['canberra'] = canberra_List\n",
        "poseinformation['chebyshev'] = chebyshev_List\n",
        "poseinformation['cityblock'] = cityblock_List\n",
        "poseinformation['correlation'] = correlation_List\n",
        "poseinformation['cosine'] = cosine_List\n",
        "poseinformation['euclidean'] = euclidean_List\n",
        "poseinformation['jensenshannon'] = jensenshannon_List\n",
        "poseinformation['mahalanobis'] = mahalanobis_List\n",
        "poseinformation['minkowski'] = minkowski_List\n",
        "poseinformation['seuclidean'] = seuclidean_List\n",
        "poseinformation['sqeuclidean'] = sqeuclidean_List\n",
        "\n",
        "poseinformation['dice'] = dice_List\n",
        "poseinformation['hamming'] = hamming_List\n",
        "poseinformation['jaccard'] = jaccard_List\n",
        "poseinformation['kulsinski'] = kulsinski_List\n",
        "poseinformation['rogerstanimoto'] = rogerstanimoto_List\n",
        "poseinformation['russellrao'] = russellrao_List\n",
        "poseinformation['sokalmichener'] = sokalmichener_List\n",
        "poseinformation['sokalsneath'] = sokalsneath_List\n",
        "poseinformation['yule'] = yule_List"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNPs6g5SVebk",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7je8fm9v_Mc",
        "colab_type": "text"
      },
      "source": [
        "# **Smoothing and Valley detection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1mKf7VjiYrW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz3-rtxU4zXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = poseinformation['mahalanobis'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KGbywuXiDyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "poseinformation['mahalanobis_minpeak'] = savgol_filter(poseinformation['mahalanobis'], window_length = 71, polyorder = 1)\n",
        "plt.plot(poseinformation.index,poseinformation['mahalanobis_minpeak'], color='red')\n",
        "\n",
        "plt.gcf().set_size_inches(30,8)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAKiCOP23WVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Detect peaks in data based on their amplitude and other features.\"\"\"\n",
        "\n",
        "from __future__ import division, print_function\n",
        "import numpy as np\n",
        "\n",
        "__author__ = \"Marcos Duarte, https://github.com/demotu/BMC\"\n",
        "__version__ = \"1.0.5\"\n",
        "__license__ = \"MIT\"\n",
        "\n",
        "\n",
        "def detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising',\n",
        "                 kpsh=False, valley=False, show=False, ax=None):\n",
        "\n",
        "    \"\"\"Detect peaks in data based on their amplitude and other features.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : 1D array_like\n",
        "        data.\n",
        "    mph : {None, number}, optional (default = None)\n",
        "        detect peaks that are greater than minimum peak height (if parameter\n",
        "        `valley` is False) or peaks that are smaller than maximum peak height\n",
        "         (if parameter `valley` is True).\n",
        "    mpd : positive integer, optional (default = 1)\n",
        "        detect peaks that are at least separated by minimum peak distance (in\n",
        "        number of data).\n",
        "    threshold : positive number, optional (default = 0)\n",
        "        detect peaks (valleys) that are greater (smaller) than `threshold`\n",
        "        in relation to their immediate neighbors.\n",
        "    edge : {None, 'rising', 'falling', 'both'}, optional (default = 'rising')\n",
        "        for a flat peak, keep only the rising edge ('rising'), only the\n",
        "        falling edge ('falling'), both edges ('both'), or don't detect a\n",
        "        flat peak (None).\n",
        "    kpsh : bool, optional (default = False)\n",
        "        keep peaks with same height even if they are closer than `mpd`.\n",
        "    valley : bool, optional (default = False)\n",
        "        if True (1), detect valleys (local minima) instead of peaks.\n",
        "    show : bool, optional (default = False)\n",
        "        if True (1), plot data in matplotlib figure.\n",
        "    ax : a matplotlib.axes.Axes instance, optional (default = None).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ind : 1D array_like\n",
        "        indeces of the peaks in `x`.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The detection of valleys instead of peaks is performed internally by simply\n",
        "    negating the data: `ind_valleys = detect_peaks(-x)`\n",
        "    \n",
        "    The function can handle NaN's \n",
        "\n",
        "    See this IPython Notebook [1]_.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] http://nbviewer.ipython.org/github/demotu/BMC/blob/master/notebooks/DetectPeaks.ipynb\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from detect_peaks import detect_peaks\n",
        "    >>> x = np.random.randn(100)\n",
        "    >>> x[60:81] = np.nan\n",
        "    >>> # detect all peaks and plot data\n",
        "    >>> ind = detect_peaks(x, show=True)\n",
        "    >>> print(ind)\n",
        "\n",
        "    >>> x = np.sin(2*np.pi*5*np.linspace(0, 1, 200)) + np.random.randn(200)/5\n",
        "    >>> # set minimum peak height = 0 and minimum peak distance = 20\n",
        "    >>> detect_peaks(x, mph=0, mpd=20, show=True)\n",
        "\n",
        "    >>> x = [0, 1, 0, 2, 0, 3, 0, 2, 0, 1, 0]\n",
        "    >>> # set minimum peak distance = 2\n",
        "    >>> detect_peaks(x, mpd=2, show=True)\n",
        "\n",
        "    >>> x = np.sin(2*np.pi*5*np.linspace(0, 1, 200)) + np.random.randn(200)/5\n",
        "    >>> # detection of valleys instead of peaks\n",
        "    >>> detect_peaks(x, mph=-1.2, mpd=20, valley=True, show=True)\n",
        "\n",
        "    >>> x = [0, 1, 1, 0, 1, 1, 0]\n",
        "    >>> # detect both edges\n",
        "    >>> detect_peaks(x, edge='both', show=True)\n",
        "\n",
        "    >>> x = [-2, 1, -2, 2, 1, 1, 3, 0]\n",
        "    >>> # set threshold = 2\n",
        "    >>> detect_peaks(x, threshold = 2, show=True)\n",
        "\n",
        "    Version history\n",
        "    ---------------\n",
        "    '1.0.5':\n",
        "        The sign of `mph` is inverted if parameter `valley` is True\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    x = np.atleast_1d(x).astype('float64')\n",
        "    if x.size < 3:\n",
        "        return np.array([], dtype=int)\n",
        "    if valley:\n",
        "        x = -x\n",
        "        if mph is not None:\n",
        "            mph = -mph\n",
        "    # find indices of all peaks\n",
        "    dx = x[1:] - x[:-1]\n",
        "    # handle NaN's\n",
        "    indnan = np.where(np.isnan(x))[0]\n",
        "    if indnan.size:\n",
        "        x[indnan] = np.inf\n",
        "        dx[np.where(np.isnan(dx))[0]] = np.inf\n",
        "    ine, ire, ife = np.array([[], [], []], dtype=int)\n",
        "    if not edge:\n",
        "        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n",
        "    else:\n",
        "        if edge.lower() in ['rising', 'both']:\n",
        "            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n",
        "        if edge.lower() in ['falling', 'both']:\n",
        "            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n",
        "    ind = np.unique(np.hstack((ine, ire, ife)))\n",
        "    # handle NaN's\n",
        "    if ind.size and indnan.size:\n",
        "        # NaN's and values close to NaN's cannot be peaks\n",
        "        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan-1, indnan+1))), invert=True)]\n",
        "    # first and last values of x cannot be peaks\n",
        "    if ind.size and ind[0] == 0:\n",
        "        ind = ind[1:]\n",
        "    if ind.size and ind[-1] == x.size-1:\n",
        "        ind = ind[:-1]\n",
        "    # remove peaks < minimum peak height\n",
        "    if ind.size and mph is not None:\n",
        "        ind = ind[x[ind] >= mph]\n",
        "    # remove peaks - neighbors < threshold\n",
        "    if ind.size and threshold > 0:\n",
        "        dx = np.min(np.vstack([x[ind]-x[ind-1], x[ind]-x[ind+1]]), axis=0)\n",
        "        ind = np.delete(ind, np.where(dx < threshold)[0])\n",
        "    # detect small peaks closer than minimum peak distance\n",
        "    if ind.size and mpd > 1:\n",
        "        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n",
        "        idel = np.zeros(ind.size, dtype=bool)\n",
        "        for i in range(ind.size):\n",
        "            if not idel[i]:\n",
        "                # keep peaks with the same height if kpsh is True\n",
        "                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n",
        "                    & (x[ind[i]] > x[ind] if kpsh else True)\n",
        "                idel[i] = 0  # Keep current peak\n",
        "        # remove the small peaks and sort back the indices by their occurrence\n",
        "        ind = np.sort(ind[~idel])\n",
        "\n",
        "    if show:\n",
        "        if indnan.size:\n",
        "            x[indnan] = np.nan\n",
        "        if valley:\n",
        "            x = -x\n",
        "            if mph is not None:\n",
        "                mph = -mph\n",
        "        _plot(x, mph, mpd, threshold, edge, valley, ax, ind)\n",
        "\n",
        "    return ind\n",
        "\n",
        "\n",
        "def _plot(x, mph, mpd, threshold, edge, valley, ax, ind):\n",
        "    \"\"\"Plot results of the detect_peaks function, see its help.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "    except ImportError:\n",
        "        print('matplotlib is not available.')\n",
        "    else:\n",
        "        if ax is None:\n",
        "            _, ax = plt.subplots(1, 1, figsize=(30, 5))\n",
        "\n",
        "        ax.plot(x, 'b', lw=1)\n",
        "        if ind.size:\n",
        "            label = 'valley' if valley else 'peak'\n",
        "            label = label + 's' if ind.size > 1 else label\n",
        "            ax.plot(ind, x[ind], '+', mfc=None, mec='r', mew=2, ms=8,\n",
        "                    label='%d %s' % (ind.size, label))\n",
        "            ax.legend(loc='best', framealpha=.5, numpoints=1)\n",
        "        ax.set_xlim(-.02*x.size, x.size*1.02-1)\n",
        "        ymin, ymax = x[np.isfinite(x)].min(), x[np.isfinite(x)].max()\n",
        "        yrange = ymax - ymin if ymax > ymin else 1\n",
        "        ax.set_ylim(ymin - 0.1*yrange, ymax + 0.1*yrange)\n",
        "        ax.set_xlabel('Data #', fontsize=14)\n",
        "        ax.set_ylabel('Amplitude', fontsize=14)\n",
        "        mode = 'Valley detection' if valley else 'Peak detection'\n",
        "        ax.set_title(\"%s (mph=%s, mpd=%d, threshold=%s, edge='%s')\"\n",
        "                     % (mode, str(mph), mpd, str(threshold), edge))\n",
        "        # plt.grid()\n",
        "        plt.show()\n",
        "        \n",
        "ind = detect_peaks(poseinformation['mahalanobis_minpeak'], mph=20, mpd=100, threshold=0, edge='rising', kpsh=False, valley=True, show=True, ax=None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0si6Ev-tKqsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Show overview of all summaries together.\n",
        "\n",
        "poseinformation['MahalanobisSummary'] = poseinformation.iloc[ind]['Imagename']\n",
        "poseinformation['UnifSamplBaseline'] = poseinformation.iloc[::100, :]['Imagename']\n",
        "poseinformation['SBDBaseline'] = poseinformation[poseinformation['Imagename'].str.replace('Thesis_GLy2rYHwUqY', 'Thesis_GLy2rYHwUqY_Complete').isin(df_HistCompare_Correlation_Cuts['Imagename'])]['Imagename']\n",
        "poseinformation[['MahalanobisSummary', 'UnifSamplBaseline', 'SBDBaseline']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uu9vxff43so",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Store results before creating actual summaries.\n",
        "mask = poseinformation['MahalanobisSummary'].notnull() | poseinformation['UnifSamplBaseline'].notnull() | poseinformation['SBDBaseline'].notnull()\n",
        "filename = 'summary' + '_' + YOUTUBE_ID + '_' + str(FRAMESTEP) + 'fts.csv'\n",
        "print(filename)\n",
        "\n",
        "poseinformation[mask].to_csv(path_or_buf='/content/drive/My Drive/Thesis/'+filename, sep=';')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Juxb79l7CDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Thanks to https://www.pyimagesearch.com/2015/09/07/blur-detection-with-opencv/\n",
        "summary2 = pd.DataFrame()\n",
        "compare = []\n",
        "finalpictures = []\n",
        "\n",
        "for number in summary2.index:\n",
        "  compareResult = []  \n",
        "  if number == 0:\n",
        "    compare = [poseinformation['Imagename'].iloc[number], poseinformation['Imagename'].iloc[number+1]]\n",
        "    #print(\"To compare:\", compare)\n",
        "  else:\n",
        "    try:\n",
        "      compare = [poseinformation['Imagename'].iloc[number-1], poseinformation['Imagename'].iloc[number], poseinformation['Imagename'].iloc[number+1]]\n",
        "      #print(\"To compare:\", compare)\n",
        "    except IndexError:\n",
        "      continue  \n",
        "  for imagePath in compare:    \n",
        "    # load the image, convert it to grayscale, and compute the\n",
        "    # focus measure of the image using the Variance of Laplacian\n",
        "    # method\n",
        "    strImage = str(imagePath).strip()\n",
        "    image = cv.imread(strImage)\n",
        "    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "    fm = cv.Laplacian(gray, cv.CV_64F).var()\n",
        "    #print(imagePath, fm)\n",
        "    compareResult.append([imagePath,fm])\n",
        "    index, value = max(compareResult, key=lambda item: item[1])\n",
        "    finalpictures.append(index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpIcxYweP2Bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "YogaSummary = poseinformation.iloc[ind]['Imagename'].tolist()\n",
        "list_im = sorted(set(YogaSummary))\n",
        "\n",
        "i = 0\n",
        "\n",
        "imgs    = [ PIL.Image.open(i) for i in list_im ]\n",
        "imgs    = [i.resize((960, 540), PIL.Image.ANTIALIAS) for i in imgs]\n",
        "min_shape = [(np.sum(i.size), i.size ) for i in imgs][0][1]\n",
        "\n",
        "imgs_comb = np.hstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
        "# save that beautiful picture\n",
        "imgs_comb = PIL.Image.fromarray( imgs_comb)\n",
        "imgs_comb.save( 'drive/My Drive/Thesis/Yoga_Horizontal_pose_steps.png', compression='png' )    \n",
        "\n",
        "# for a vertical stacking it is simple: use vstack\n",
        "imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
        "imgs_comb = PIL.Image.fromarray( imgs_comb)\n",
        "imgs_comb.save( 'drive/My Drive/Thesis/Yoga_Vertical_pose_steps.png', compression='png' )    \n",
        "Pose_Labelling_End = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-rLnOiEZ4eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create SBD Baseline summary\n",
        "SBDBaselineSummary = poseinformation['SBDBaseline'][poseinformation['SBDBaseline'].notnull()].tolist()\n",
        "list_im = sorted(set(SBDBaselineSummary))\n",
        "\n",
        "i = 0\n",
        "\n",
        "imgs    = [ PIL.Image.open(i) for i in list_im ]\n",
        "imgs    = [i.resize((960, 540), PIL.Image.ANTIALIAS) for i in imgs]\n",
        "min_shape = [(np.sum(i.size), i.size ) for i in imgs][0][1]\n",
        "\n",
        "imgs_comb = np.hstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
        "# save that beautiful picture\n",
        "imgs_comb = PIL.Image.fromarray( imgs_comb)\n",
        "imgs_comb.save( 'drive/My Drive/Thesis/SBDBaseline_Horizontal_pose_steps.png', compression='png' )    \n",
        "\n",
        "# for a vertical stacking it is simple: use vstack\n",
        "imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
        "imgs_comb = PIL.Image.fromarray( imgs_comb)\n",
        "imgs_comb.save( 'drive/My Drive/Thesis/SBDBaseline_Vertical_pose_steps.png', compression='png' )    \n",
        "Pose_Labelling_End = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYNPTKbSawG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create uniform sampling baseline summary\n",
        "UnifSamplBaselineSummary = poseinformation['UnifSamplBaseline'][poseinformation['UnifSamplBaseline'].notnull()].tolist()\n",
        "list_im = sorted(set(UnifSamplBaselineSummary))\n",
        "\n",
        "i = 0\n",
        "\n",
        "imgs    = [ PIL.Image.open(i) for i in list_im ]\n",
        "imgs    = [i.resize((960, 540), PIL.Image.ANTIALIAS) for i in imgs]\n",
        "min_shape = [(np.sum(i.size), i.size ) for i in imgs][0][1]\n",
        "\n",
        "imgs_comb = np.hstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
        "# save that beautiful picture\n",
        "imgs_comb = PIL.Image.fromarray( imgs_comb)\n",
        "imgs_comb.save( 'drive/My Drive/Thesis/UnifSamplBaseline_Horizontal_pose_steps.png', compression='png' )    \n",
        "\n",
        "# for a vertical stacking it is simple: use vstack\n",
        "imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
        "imgs_comb = PIL.Image.fromarray( imgs_comb)\n",
        "imgs_comb.save( 'drive/My Drive/Thesis/UnifSamplBaseline_Vertical_pose_steps.png', compression='png' )    \n",
        "Pose_Labelling_End = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}